\section{Introduction}

  In a popular benchmarking ``of all publicly accessible
  implementations of convnets,''
  (https://github.com/soumith/convnet-benchmarks) all performance
  numbers are for GPUs, with none given for CPUs.  The popularity of
  GPUs for deep learning has resulted from both hardware and software
  advantages.  The hardware gap has narrowed recently, as the Knights
  Landing (KNL) Xeon Phi CPU delivers up to 6 TFLOPS, compared to the
  11 TFLOPS rating of the Titan X Pascal GPU (all FLOPS ratings are
  given for single precision arithmetic).

  There is also a software gap.  All the major deep learning
  frameworks use the cuDNN library, which has been developed and
  refined for several years and is already at version 5.  cuDNN
  achieves extremely high percentage of utilization of available
  FLOPS, at least for some select convolutional network (ConvNet)
  architectures~\cite{imagenetwinners}.

  This paper focuses on CPU implementation of ConvNets that makes
  efficient use of available FLOPS.  A ConvNet contains convolutional
  and other kinds of layers.  A convolutional layer requires an amount
  of computation that grows with the square of the number of 
  images, whereas other kinds of layers scale linearly.  Therefore
  almost all of the computation in a typical ConvNet resides in the
  convolutional layers, which are the subject of the current paper.

  A popular approach is to implement convolution using
  matrix multiplication
  libraries~\cite{chellapilla2006high}.  This provides speedup for
  diverse ConvNet architectures, and has been applied to both
  GPUs~\cite{chetlur2014cudnn,neonnervana} and CPUs
  ~\cite{hadjis2015shallow}.  But the efficiency of the approach
  suffers because of computational and memory overhead due to the
  requirement of creating in--memory matrices.

  Another popular approach is to use meta--programming in various
  forms.  In offline meta--programming, the
  primitives for solving a sub--problem are either optimized
  by hand or by code generators.  This approach has been popularized
  by FFTW~\cite{frigo1998fftw,frigo1999fftw} and adopted by some some
  ConvNet frameworks~\cite{klockner2012pycuda,nervanagpu}.  This can
  achieve the highest efficiency of all approaches, but is
  laborious and the speed-up may not generalize across ConvNet architectures
  (just as FFTW is efficient only
  for certain sizes).  In fact, many GPU and CPU deep learning
  implementations appear to optimize code for standard benchmark
  ConvNet architectures, for which they achieve markedly higher
  efficiency than on other architectures (as demonstrated in this paper).

  A run--time form of meta--programming has
  been used by Intel in its MKL-DNN package.  Given a ConvNet
  architecture, the MKL-DNN software generates optimized assembly code
  at run time.  The part of the current package that is
  open--sourced~\cite{idlf,mkl-dnn} is for Broadwell and Haswell CPUs,
  and it is unclear how easily it generalizes to other CPUs including
  KNL.

  Here we propose a compile--time form of meta--programming
  that aims to be general with respect to both the ConvNet
  architecture and CPU.  We use C++ templates to create primitives at
  compile time.  Given the specifications of the CPU, our
  meta--algorithms optimize and vectorize single-threaded code, as
  well as generate static scheduling for parallelizing the computation
  over multiple cores.  Our approach tries to achieve generality by
  leveraging the power of the compiler.

%  To our knowledge this is the first application of compile--time
%  meta--programming to ConvNets.

  We show that our approach can utilize high percentage of the
  available FLOPS regardless of the CPU generation, as well as have
  near linear scalability with the number of cores.  We are
  competitive with state-of-the-art hand-optimized code for specific
  architectures.  Our code yields better performances on KNL
  compared to the state of the art GPU implementations for 3D
  convolutional networks, even though KNL has less FLOPS available.

  FFT--based algorithms for training and inference of ConvNets have
  been introduced for both the CPU
  ~\cite{zlateski2016znn,zlateski2016znni} and the
  GPU~\cite{mathieu-iclr-14,vasilache2014fast}.  These are only
  efficient for large kernel sizes, while small kernel sizes are
  currently much more popular
  ~\cite{szegedy2015going,ronneberger2015u, simonyan2014very,
    sermanet2013overfeat, long2015fully, tran2015learning, ji20133d,
    maturana_iros_2015, maturana_icra_2014}.

  \subsection{Related work}

  Our work is most closely related to Refs.
  \cite{chellapilla2006high,das2016distributed}, which propose
  ``guidelines'' for parallelization and vectorization on Intel CPUs.
  We adopt their recommended memory layout, in which data that will be
  loaded into a vector register is kept contiguous and aligned.  Our
  \emph{lowest-level} serial primitive for forward/backward
  propagation employs the recommended vectorization strategy, except
  that the loops are nested in a slightly different order.  This is
  where the similarity ends.  By using a stack of primitives, our
  serial algorithm performs computation in a nontrivial way that
  maximizes L2/L3 hit rate, but is oblivious to the cache size.
  While~\cite{chellapilla2006high} has to be manually tuned for
  different kernel sizes, our approach is generic---register and L1
  blocking strategy is computed during compile-time for arbitrary
  $N$-dimensional image and kernel sizes.

  Our lowest-level serial primitive for the kernel update employs a
  new algorithm with a novel vectorization strategy. This contrasts
  with the approach recommended by Ref.~\cite{chellapilla2006high},
  which is to modify the forward/backward propagation algorithm.

  For parallelization across cores,  ~\cite{chellapilla2006high} and~\cite{das2016distributed} describe a simple fork--join method, where the
  problem is divided into very fine--granularity tasks.
  This method is suited only for specific networks, where the batch
  size or the number of output images is large.  It does not scale
  well for an arbitrary problem size and arbitrary number of cores.
  Our statically scheduled approach scales to arbitrary network
  topology and number of cores, by evenly dividing the work among
  available cores.


  %% http://legion.stanford.edu/pdfs/bootcamp2014/10_metaprogramming.pdf
