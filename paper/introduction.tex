\section{Introduction}

  ConvNets had become... popular \mynote{seung}{intro about convnets}.
  The argument of using CPUs or GPUs for deep learning had reached
  almost religious proportions.  The GPUs, which are capable of more
  FLOPs/s seem to win the battle by a large margin -- disproportional
  to the FLOPs/s available to the two architectures.  We find this
  absurd, as the CPUs have been around for much longer, and the CPU
  programming model changes very little from a generation to the next.
  On the other side, GPUs seem to change a lot.

  As the CPUs are closing the gap in terms of FLOPs/s performance.
  Xeon Phi Knights Landing chips are capable of up to
  6TFLOPs/s~\cite{}.  Upcoming Skylake server CPU's will be packing
  $3.2$ TFLOPs/s per single chip, delivering up to $25.8$ TFLOPs/s on
  an 8-way configuration~\cite{}.  We believe that the main battle
  should be in harvesting the computational power of the hardware.  We
  are surprised to see that the GPU implementations have higher
  utilization of the \% FLOPs/s available, compared to the CPU.

  The amount of computation required by the convolutional layers of a
  ConvNet grows as a square of the number of channels (images).  In
  contrast, other types of layers require linear amount of operations
  per output image.  State of the art networks have convolutional
  layers with anywhere from $64$ to $1024$ images.  This means that
  $98.5\%$ up to $99.9\%$ of the computation is in the convolutional
  layers.  It is therefore most important to speed up these layers.

  Reducing convolution to matrix multiplication has been a popular
  approach due to the availability of highly efficient matrix
  multiplication routines.  This approach has, however, both a
  computational and memory overhead due to the requirement of creating
  in--memory matrices.  Caffe con Troll (CCT)~\cite{hadjis2015shallow}
  uses blas to perform convolutions -- we will compare us to them.

  Initially, cuDNN~\cite{chetlur2014cudnn} took that approach,
  however, it has since implemented more efficient direct convolution
  algorithms.

  Convolutional layers of a ConvNet are computationally dominating
  both training and inference.  The forward and backward pass involve
  a convolution of an image with a small kernel.  Typically ``valid''
  dense convolution, possibly with strides is performed.  More
  advanced types of convolutions can decomposed to a series of dense
  convolutions~\cite{szegedy2015going}.

  We propose a new approach.  We harvest the power of C++ templates to
  create compile time primitives by harvesting the compiler.  Given
  the specifications of the CPU, our meta--algorithms optimize and
  vectorize single threaded code, as well as generate static
  scheduling for parallelizing the computation over multiple cores.

  We show that our approach can utilize extremely high percentage of
  the available FLOPS regardless of the CPU generation, as well as
  have near linear scalability over the number of cores available.
  The performances of our algorithms are competitive with the state of
  the art hand optimized code for specific architectures.  Our code
  yields better performances on the KNL compared to the state of the
  art GPU implementations for 3D convolutional networks, even though
  KNL has less FLOPS/s available.

  We further demonstrate the power of our approach for inference only
  computation.

  Even though we provide an efficient, publicly available, open source
  implementation for popular ConvNet primitives, the goal of this
  paper is to point to the right direction for solving the problem,
  rather than giving the final ``optimized'' implementation.

  Efficient algorithms for training and inference of ConvNets with
  larger kernels have been introduced in
  ~\cite{zlateski2016znn,zlateski2016znni}, however the evidence
  suggests that ConvNets with smaller kernels, and larger number of
  layers perform better on nearly all problems. (Cite).

  %% \subsection{GPUs vs CPUs}

  %% GPUs have streaming multiprocessors, while the CPUs have in--order,
  %% memory coherent cores.

  %% GPUs model is so called SIMT (single instruction multiple
  %% threads)... dwell on it..

  %% CPUs on the other side have a complex structure of many cores and
  %% virtual cores, many cache layers and each core capable of performing
  %% SIMD instructions on up to 16 floating point numbers.

  %% In this paper we are not trying to answer the GPU vs CPU question,
  %% but rather propose an algorithm and implementation for ConvNet
  %% primitives that can utilize very high percentage of the theoretical
  %% peak performance of both many--core (Xeon Phi) and multi--core
  %% (Xeon) CPUs.  As shown later, our algorithm performs well on
  %% multi--chip systems as well.

  %% \subsection{Mulit--core vs Many--core}

  %% Explain the differences NUMA, caches, etc..


  \aleks{Summarize and cite the text below}
  A popular convolution implementation is to unroll the data until the
  computation is in the form of a large matrix multiplication
  (Chellapilla et al. (2006)). This is the strategy followed by many
  implementors, since matrix multiplication is a well-tuned linear
  algebra primitive available on virtually any platform. While it is
  possible to provide instances of direct calculation that are faster
  than matrix unrolling (e.g., for large S, Krizhevsky (2014)), it is
  challenging to provide an implementation that is faster for more
  than just a small subset of possible convolution problems.
