\section{Introduction}

  In a popular benchmarking ``of all publicly accessible
  implementations of convnets,''
  (https://github.com/soumith/convnet-benchmarks) all performance
  numbers are for GPUs, with none given for CPUs.  The popularity of
  GPUs for deep learning has resulted from both hardware and software
  advantages.  The hardware gap has narrowed recently, as the Knights
  Landing (KNL) Xeon Phi CPU delivers up to 6 TFLOPS, compared to the
  11 TFLOPS rating of the Titan X Pascal GPU (all FLOPS ratings are
  given for single precision arithmetic).

  There is also a software gap.  All the major deep learning
  frameworks use the cuDNN library, which has been developed and
  refined for several years and is already at version 5.  cuDNN
  achieves extremely high percentage of utilization of available
  FLOPS, at least for some select convolutional network (ConvNet)
  architectures~\cite{imagenetwinners}.

  This paper focuses on CPU implementation of ConvNets that makes
  efficient use of available FLOPS.  A ConvNet contains convolutional
  and other kinds of layers.  A convolutional layer requires an amount
  of computation that grows with the square of the number of channels
  (images), whereas other kinds of layers scale linearly.  Therefore
  almost all of the computation in a typical ConvNet resides in the
  convolutional layers, which are the subject of the current paper.

  An highly popular approach is to implement convolution using highly
  efficient matrix multiplication
  libraries~\cite{chellapilla2006high}.  This provides speedup for
  diverse ConvNet architectures, and has been applied to both
  GPUs~\cite{chetlur2014cudnn,neonnervana} and CPUs
  ~\cite{hadjis2015shallow}.  But the efficiency of the approach
  suffers because of computational and memory overhead due to the
  requirement of creating in--memory matrices.

  Another popular approach is to use meta--programming in various
  forms.  One approach is to use offline meta--programming, where the
  optimal primitives for solving a sub--problem are either optimized
  by hand or by code generators.  This approach has been popularized
  by FFTW~\cite{frigo1998fftw,frigo1999fftw} and adopted by some some
  ConvNet frameworks~\cite{klockner2012pycuda,nervanagpu}.  This can
  achieve the highest efficiency of all approaches, but is
  time-consuming and the speed-up does not generalize beyond the
  specific or similar architectures.  (just as FFTW is efficient only
  for certain sizes).  In fact, many GPU and CPU deep learning
  implementations appear to contain hand-optimized code for standard
  benchmark network architectures, for which they achieve markedly
  higher efficiency than is typical (as demonstrated in this paper)

  Another form of meta--programming, run--time meta--programming has
  been introduced by intel in its MKL-DNN package.  Given a ConvNet
  architecture, the MKL-DNN software generates optimized assembly code
  at run time.  The part of the current package that is
  open--sourced~\cite{idlf,mkl-dnn} is for Broadwell and Haswell CPUs,
  and it is unclear how easily it generalizes to other CPUs including
  KNL.

  We propose another form of, compile--time, meta--programming
  approach that aims to be general with respect to both the ConvNet
  architecture and CPU.  We use C++ templates to create primitives at
  compile time.  Given the specifications of the CPU, our
  meta--algorithms optimize and vectorize single-threaded code, as
  well as generate static scheduling for parallelizing the computation
  over multiple cores.  Our approach tries to achieve generality by
  leveraging the power of the compiler.

  To our knowledge this is the first application of compile--time
  meta--programming to ConvNets.

  We show that our approach can utilize high percentage of the
  available FLOPS regardless of the CPU generation, as well as have
  near linear scalability with the number of cores.  We are
  competitive with state-of-the-art hand-optimized code for specific
  architectures.  Our code yields better performances on KNL
  compared to the state of the art GPU implementations for 3D
  convolutional networks, even though KNL has less FLOPS/s available.

  Even though we provide an efficient, publicly available, open source
  implementation for popular ConvNet primitives, the goal of this
  paper is to point to the right direction for solving the problem,
  rather than giving the final ``optimized'' implementation.

  FFT--based algorithms for training and inference of ConvNets have
  been introduced for both the CPU
  ~\cite{zlateski2016znn,zlateski2016znni} and the
  GPU~\cite{mathieu-iclr-14,vasilache2014fast}.  These are only
  efficient for large kernel sizes, while small kernel sizes are
  currently much more popular
  ~\cite{szegedy2015going,ronneberger2015u, simonyan2014very,
    sermanet2013overfeat, long2015fully, tran2015learning, ji20133d,
    maturana_iros_2015, maturana_icra_2014}.

  %% http://legion.stanford.edu/pdfs/bootcamp2014/10_metaprogramming.pdf
