\section{Conclusion and closing remarks}

  Based on the experiments conducted, we conclude that our
  compile--time meta--programming approach to optimize ConvNets yields
  very high utilization and high scalability over large number of
  cores regardless of the ConvNet type.

  This approach is very easy to implement using C++ templates.  Our
  implementation, which is publicly available at
  \url{https://github.com/seung-lab/znnphi-release}, consist of around
  1000 lines of C++ code for each of two algorithms.  The
  implementation, however, is not meant to be used as a final product.
  Future work will include creating a production ready code, as well
  as integrating the primitives into popular ConvNet frameworks such
  as Caffe.

  Additional benchmarks on AVX and SSE4 CPUs, achieving roughly the
  same utilization, were performed but not reported as these older
  generations of CPUs were capable of much less FLOPS.  In practice,
  such CPUs would never be used for training ConvNets.  However,
  consumer applications that use already trained ConvNets, such as
  image processing apps, can greatly benefit from our approach by
  efficiently running the forward propagation on older CPU
  generations.

  Finally, sliding window inference, which is important for
  segmentation problems, can greatly benefit from our approach as
  access to more RAM can greatly increase the inference
  throughput~\cite{zlateski2016znni}.  In this cases we expect our
  approach to over--perfom the GPUs, even for the 2D cases, as
  generally CPUs have access to much more RAM.
