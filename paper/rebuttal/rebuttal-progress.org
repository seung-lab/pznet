R3 and R4 pointed out notes-to-self and missing references.  We now
realize that we uploaded an outdated version of the paper in the final
rush to submit. This mistake is easily corrected. More generally, we
expect to greatly strengthen our paper by addressing the wonderfully
detailed and thoughtful criticisms of the reviewers.

We will release the source code upon publication.

R1 requested a clear statement of novelty relative to [25] and
https://arxiv.org/pdf/1602.06709v1.pdf.  [25]provides vectorization
strategies and 'guidelines' for both serial and parallel algorithms.
It suggests specific register blocking for certain kernel sizes.  Our
forward pass' lowest level primitive employs the same vectorization
strategy, but performs computation in a different order.  This is
where the similarity ends.  Our lowest-level update primitive has
different blocking strategy.  Our serial algorithms perform
computation in a nontrivial way that maximizes L2/L3 hit rates.  Our
register and L1 blocking strategy is computed during compile-time for
an arbitrary N-dimensional kernel size.

[25] also outlines a simple fork-join algorithm for specific networks
(p.566/567).  Our statically scheduled approach scales to arbitrary
network topology and kernel sizes.

https://arxiv.org/pdf/1602.06709v1.pdf focuses on distributing
training over multiple nodes, using the same serial and parallel
algorithms as in [25].

** DONE ALL ABOVE

* R1 questions

** TODO 0)

  Intrinsics are indeed used (see page 4).  We will make that clearer.

** DONE a)

  We will mention that MKL-DNN is only optimized for forward and AVX2.

** DONE b)

  1 vs 2 threads depends on whether it is more beneficial to hide
  latency or have more cache per thread available.  Here, many factors
  such as blocking size, kernel sizes, available cache, etc...  come
  in play.  For this reason, we empirically determine the optimal
  choice.

** TODO c)

  Similar performances are obtained for 7x7/5x5 of GoogLeNet/Resnet.
  We'll add extra "toy-layers" to cover those cases.

** DONE d)

  Extra analysis of the benchmarks will be added.  NUMA affects
  scalability for some cases on Haswell.  KNL has linear scalability
  in nearly all cases, with the exception of Unet-update, which is due
  to relatively low amount of FLOPS required and subdivision into
  non-exactly-equal size problems.

  On our NUMA Haswell machine GEMM scales worse, not better.  On KNL
  our algorithm scales linearly in nearly all cases, with the
  exception of UNet which can be considered a harder case.  Similarly,
  GEMM has hard cases with bad scaling (small/narrow matrices).

  One of the strengths of our parallelization method is that no
  special handling of NUMA is required.  Good performance was achieved
  with efficient utilization of L3.

** DONE e)

  Insufficient vectorization, not enough loop unrolling and lack of L3
  all contribute to relatively lower utilization of KNL versus
  Haswell.  We expect the first two problems to be solved as
  compilers' optimization of KNL code matures.  We still expect CPUs
  with L3 to have higher utilization on average.

** TODO f)

  Such analysis would be useful, but the MKL-DNN authors should do it.
  Lack of clear documentation or publication on the exact approach
  makes it harder for us to do the analysis.

** TODO g)

  Our C++ metaprogramming approach could be regarded as a JIT approach
  as well, with the C++ compiler doing the JIT compilation.  Our
  approach has multiple advantages:

  Our code compiles for KNL/Haswell/SandyBridge/Westmere/... (and we
  expect future generations) with the only customization being 6
  macros and 4 constants.  Instructions are optimally scheduled due to
  the outstanding work of the compiler community.  The JIT approach of
  [NNPACK,MKL] is less portable because it requires replicating much
  of the work done by the compiler - such as optimal register usage,
  instruction scheduling, etc..

** TODO h)

  NNPack is not scalable to high number of cores, based on our own
  experiments and personal conversation with the NNPack author.

** DONE i)

  We plan to publish results for DeepBench on different CPU
  generations after the release of the code.

* R2 questions

** CANCELED 1)

  Indeed convolution can be viewed as matrix multiplication.  However,
  a convolution matrix has a specific structure, which should be
  exploited to achieve higher efficiency.  This is why, for example,
  the GEMM approach has been superseded in recent cuDNN versions (2D
  convolution).

** DONE 2)

  UNet is the only architecture that is noticeably worse. Some of the
  reasons were given above and will be explained in the revised
  manuscript (with further analyses).

** DONE 3)

  Our Haswell machine had 4 CPUs with total of 72 cores.  It was
  capable of approximately 30% more FLOPS than our KNL machine
  (Tbl.2).

* R4 questions

** TODO

  R4 is correct that FFT/Winograd implementations can be effective for
  surprisingly small kernels.  Our previous FFT work was only
  effective for rather large kernels because we did not adequately
  exploit vectorization.  We plan to add vectorization but this is
  outside the scope of the current paper.
