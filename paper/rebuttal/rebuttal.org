* Intro to rebuttal

  - Apology for uploading wrong revision
  - Will open source the code when published, but can share with the
    reviewers

* REVIEW 1

  In this work the authors propose an implementation of n-dimensional
  convolution kernels for CNNs on x86 CPUs with AVX2 and
  AVX512. Instead of relying on a vendor library, the authors leverage
  compile-specialization through modern C++ compilers to generate
  optimal code (= optimal vectorization) via expression templates. The
  authors demonstrate in a short performance evaluation that they can
  achieve competitive performance results compared to the
  closed-source MKL library and the open-source MKL-DNN library on
  both Xeon and Xeon Phi machines.

  zlateski: Need to mention that optimal code = optimal
  vectorization + optimal static scheduling (parallelization)

  Unfortunately, the paper stays high-level and doesn't discuss
  potentially different strategies in detail. Also the code is, in
  contrast to the claim in the paper, not available open source on
  github (it's claimed it will be soon), which makes some paragraphs
  of the paper hard to follow; e.g. how are the different register
  blocking strategies implemented. Are intrinsic used to enforce
  vectorization in the inner-most level, etc.? Also the
  differentiation to work presented in [25] and
  https://arxiv.org/pdf/1602.06709v1.pdf (not cited) is not clearly
  captured. The reviewer would like to see a step-by-step code
  optimization process: we start from the simple seven nested loops
  for direct convolution and apply step-by-step optimizations a)-z)
  and these are the achieved performance improvements. That would
  allow the IPDPS community to generalize the findings. Even without
  the aforementioned detailed performance analysis the current
  analysis is very short and also high-level without !  discussing the
  reasons for the obtained performance. Even not the optimal thread
  configuration for each case is provided.

  It would nice to extend the discussion of the approach taken in this
  work vs. the MKL-DNN approach wrt. to ease of use, especially as the
  authors claim it's unclear how such an approach (JIT) carries
  forward. The reviewer believes it is not important how the code is
  generated (JIT or static with compiler) as a JIT generator could
  implement all the ideas presented in this work, so why is static
  compilation favorable over JIT? E.g. for JIT a user just links
  against a library and gets the optimal performance, for
  compiler-based optimization it might be necessary that everything
  needs to be recompiled when changing the minibatch size. Such a
  discussion is needed especially as JIT seems to establish itself as
  the tool to go, even for GPUs: https://github.com/naibaf7/libdnn or
  e.g. NNPack for CPUs: https://github.com/Maratyszcza/NNPACK.

  Therefore, the paper presents many interesting aspects on how to
  generate optimal code for direct convolutions, but some more work is
  needed to smooth out rough edges.

** Questions to authors

   0) Are intrinsic used to enforce vectorization in the inner-most
      level, etc.?

      Correct, it is mentioned on page 4.  We will make it clearer.

   0) Also the differentiation to work presented in [25] and
      https://arxiv.org/pdf/1602.06709v1.pdf (not cited) is not
      clearly captured.

      [25] proposes serial algorithms (not explained in detail, and
      with incorrect pseudocode).  It only provides "guidelines" for
      both blocking of specific 2D kernel sizes.  Proposes simple
      parallelization that is not suitable for an arbitrary network
      topology.

      Our approach generalizes the serial algorithm to 3D and
      arbitrary kernel sizes.  The forward algorithm resembles the one
      in [25], while the backward and update algorithms differ.

      We don't follow the proposed parallelization strategy but rather
      propose novel method based on statically scheduling execution,
      and breaking up the problem into tasks that don't necessary have
      the same sizes.

      [link] closely follows the serial algorithms proposed in [25]
      (probably same authors).  It also doesn't give details about the
      update algorithm which is more complex. Also, only simple
      parallelization strategy is proposed (over batch and output
      featuremaps), and no details about the implementation is given.
      Also, main point of [link] is distribution.

   0) The reviewer believes it is not important how the code is
      generated (JIT or static with compiler) as a JIT generator could
      implement all the ideas presented in this work, so why is static
      compilation favorable over JIT?

      In theory, generating the code using JIT or statically with the
      compiler should not affect the ideas presented.  Also, our
      approach can be seen as JIT as well, with C++ doing the
      compilation instead of the programmer using JIT library.

      However, Using c++ metaprogramming has multiple advantages.

      - Single source code can be compiled for various machines.  This
        makes the code easier to maintain.  Porting to a new
        generation of a CPU or different platform requires changing 6
        macros and 3 constants.  JIT proposed in [NNPACK, MKL] is much
        harder to port, and works only on AVX2 capable machines.
      - No need to re-invent the wheel - work done by the compiler
        community.  No detailed knowledge is necessary.
      - Producing the same quality code, as the compiler, is hard and
        time consuming.  An example would be aligning instructions to
        cache-line boundary, that we consistently observe in our code,
        and is not present in MKL-DNN.  Even though optimal
        compilation of AVX512 code was not achieved by all compilers,
        we expect this to change and surpass the manual JIT for KNL as
        it is for AVX2.

   a) MKL-DNN is only available with AVX2 code for forward, backward
      runs optimized fallback code, but not heavily tuned. The authors
      should mention that.

      Yes, we should mention that.

   b) the pinning strategy is not clear, when are 1 and when are 2
      threads per core used?

      The optimal number of threads per core is obtained empirically.
      Using 1 vs 2 threads depends on whether it is more beneficial to
      hide latency or have more cache available per thread.  Many
      factors contribute to whether latency or cache is more
      important, such as blocking size, kernel sizes, etc...  For this
      reason, it is best to empirically determine the optimal choice.

   c) the chosen layers have all large feature maps (due to the focus
      on image segmentation?!) which results in easy register
      blocking. The authors should add layers from topologies such as
      Googlenet or Resnet to also show the achievable performance on
      these smaller input sizes (e.g. 7x7, 5x5 feature maps sizes).

      We'll consider adding extra "toy-layers" to cover that cases.
      Similar performances are obtained for both 7x7 and 5x5 of
      Googlenet and Resnet.  Note that both 7x7 and 5x5 can also be
      easily register blocked.

   d) For Haswall and Knight Landing scaling issues are shown in Fig.5
      but not analyzed. Direct convolutions have a better FLOP/byte
      ratio than GEMM, but this code seems to scale worse than
      GEMM. This needs analysis. E.g. how is NUMA handled in case of
      the four-way Haswell system? Also a single core performance
      comparison to MKL would be helpful would be helpful.

      We agree that extra analysis should be introduced in the
      manuscript, however, I wouldn't call Fig.5 "issus", the results
      are good.  Maybe a bit more about UNet (division ends up having
      different sizes).

      - Just as in the GEMM case, depending on the network
        architecture (matrix size) the scalability is slightly better
        or worse.  For easier cases we see linear scalability, and
        harder cases are more similar to GEMM on narrow matrices...

      - One of the strengths of our parallelization method is that no
        special handling of NUMA is required.  The algorithm is
        designed to take advantage of higher level caches when
        available.  Good performances on Haswell were due to efficient
        reuse of L3 cache.

      - Agreed that it should be beneficial (lack of time to do it).

   e) why is ZNNphi able to outperform MKL on Haswell and not on
      Knights Landing? This needs discussion (VGG-A). Is it
      insufficient vectorization or non-optimal memory accesses?

      - Insufficient vectorization, and not enough unrolling (measured
        memory hit rate is extremely high).  Order of instructions
        makes vector units stall.  Manually reordering the
        instructions improves the performances.  We expect this to
        change as compilers get more mature in working for KNL.

   f) especially for large images ZNNphi seems to be much better than
      MKL, as MKL-DNN is open source an analysis why this is the case
      would be very help full. The review suspects that MKL-DNN is
      skipping one blocking. Even a fix to MKL-DNN would be great.

      - We agree, MKL-DNN is under active development, and promises to
        be as efficient as MKL2017 - it's intel's job to fix stuff!
        Also JIT is much harder to debug and analyze than statically
        compiled code.

   g) addressing JIT vs. compile-time static code generation aspects
      raised above.

      - Addressed above

   h) what about a Winograd implementation (which should help a lot
      with VGG-A) or a comparison to NNPack

      Unfortunately NNPack is not scaleable to high number of cores.
      Through experiments and direct conversation with the author.
      Only useful for forward/backward (not update).  Should mention
      that the update algorithm is more complex.  We do expect that
      for certain sizes (namely 3x3) can outperform the direct method
      for the fwd/bwd.

   i) performance results for DeepBench would be also very helpful as
      this paper only focuses on convolution layer performance not a
      full time-to-train score:
      https://github.com/baidu-research/DeepBench

      - we plan to do it

* REVIEW 2

  1) Most of the techniques discussed in the paper have been used in
     the optimization of matrix-matrix multiplication kernels and some
     tensor contraction work. What are some unique features of CNN
     forward/backward propagation that require special attention?

     not sure i understand the question...

  2) The authors claim the techniques they developed is applicable to
     all ConvNet architectures. However, from the Experiments section,
     it is clearly that some Conv Nets benefit more from their
     techniques than others. Perhaps they can comment on how CNN
     architecture can affect the performance, and what are the main
     factors that can affect the effectiveness of their optimization
     techniques.

     need to mention - in a nice way - that ~50% utilization is good
     utilization, especially when the competition is much worse.
     Additional analysis will be added to explain why different
     networks/CPUs scale differently.  Mainly - easier parallelization
     (just over batches) vs harder.  For update, need to consider the
     reduce phase.

  3) It seems that the performance of KNL lags behind that of Haswell
     even though KNL has more cores/threads and higher memory
     bandwidth. Can the authors comment on why this is the case and
     whether additional optimization can help boost the performance on
     KNL?

     Our Haswell machine had actually more cores, and Table.II.  The
     algorithm is compute bound in both cases (higher bandwidth
     doesn't help)

* REVIEW 3 & 4

  The paper needs one additional editing pass (e.g. there is a comment
  "not sure what to cite :(" on the first page that surely was meant
  to be removed before submission, and elsewhere there are empty
  citations).  But otherwise, I recommend it -- convolutional neural
  networks have taken over in image processing, and these types of
  layout tricks seem well-positioned to accelerate a lot of
  state-of-the-art packages.

  zlateski: Explain the frustration....
