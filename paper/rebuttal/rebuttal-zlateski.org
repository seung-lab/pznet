R3 and R4 pointed out notes-to-self and missing references.  We now
realize that we uploaded an outdated version of the paper in the final
rush to submit. This mistake is easily corrected. More generally, we
expect to greatly strengthen our paper by addressing the
wonderfully detailed and thoughtful criticisms of the reviewers.

We will release the source code upon publication.

================
R1 requested a clear statement of novelty relative to [25] and
https://arxiv.org/pdf/1602.06709v1.pdf.
[25] provides vectorization strategies and 'guidelines' for both
serial and parallel algorithms.  It suggests specific register
blocking for certain kernel sizes.  Our forward pass' lowest level
primitive employs the same vectorization strategy, but performs
computation in a different order.  This is where the similarity ends.
Our lowest level update primitive has different blocking strategy.
Our serial algorithms perform computation in a nontrivial way that
maximizes L2 and L3 cache hit rates.  Our register and L1
blocking strategy is computed during compile--time for an arbitrary
N-dimensional kernel size.

[25] also outlines a simple fork--join algorithm
for specific networks (p.566/567).  Our statically
scheduled approach scales to arbitrary network topology and
kernel sizes.

https://arxiv.org/pdf/1602.06709v1.pdf focuses on distributing
training over multiple nodes, using the same serial and parallel
algorithms as in [25].

0)Intrinsics are indeed used (see page 4).  We will make
  that clearer.

a)We will mention that MKL-DNN is only optimized for forward and AVX2.

b)1 vs 2 threads depends on whether it is more beneficial to
  hide latency or have more cache per thread available.  Here, many
  factors such as blocking size, kernel sizes, available cache, etc...
  come in play.  For this reason, we empirically determine
  the optimal choice.

c)Similar performances are obtained for both 7x7 and 5x5 of GoogLeNet
  and Resnet.  We'll add  extra "toy-layers" to cover those cases.

d)Extra analysis of the benchmarks will be added.
  Generally NUMA affects scalability for some cases on Haswell.
  KNL has linear scalability in nearly all cases, with the exception
  of Unet update, which is due to relatively low amount of FLOPS
  required and subdivision into non--exactly--equal size problems.

  On our NUMA Haswell machine GEMM scales worse, not better.  On KNL
  our algorithm scales linearly in nearly all cases, with the
  exception of UNet which can be considered a harder case.  Similarly,
  GEMM has hard cases with bad scaling (small or narrow
  matrices).

  One of the strengths of our parallelization method is that no
  special handling of NUMA is required.  Good performance on our NUMA
  Haswell machine is due to efficient utilization of L3 cache.

e)Insufficient vectorization, not enough loop unrolling and lack of L3
  cache all contribute to relatively lower utilization of KNL versus
  Haswell.  We expect the first two problems to be solved as
  compilers' optimization of KNL code matures.  We still expect CPUs
  with L3 cache to have higher utilization on average.

f)Such analysis would be useful, but the MKL-DNN authors should do it.
  Lack of clear documentation or publication on the exact approach
  makes it harder for us to do the analysis.

g)Our C++ metaprogramming approach could be regarded as a JIT approach
  as well, with the C++ compiler doing the JIT compilation.  Our approach
  has multiple advantages:

  Our code compiles for Haswell and KNL (and we expect future CPU
  generations) with the only customization being 6 macros and 3
  constants.  Instructions are optimally scheduled due to the
  outstanding work of the compiler community.  The JIT approach of
  [NNPACK, MKL] is less portable because it requires replicating much
  of the work done by the compiler community - such as optimal
  register usage, instruction scheduling, etc..

h) NNPack is not scalable to high number of cores, based on our own
   experiments and personal conversation with the NNPack author.  

i)We plan to publish results for DeepBench on different CPU
  generations (SSE-AVX512) after the release of the code.

===============
R2 questions:

1)Indeed convolution can be viewed as matrix multiplication.  However,
a convolution matrix has a specific structure, which should be
exploited to achieve higher efficiency.  This is why, for example, the
GEMM approach has been superseded in recent cuDNN versions (for 2D
convolution).

2)UNet is the only architecture that is noticeably worse. Some of
the reasons were given above and will be explained in the revised manuscript (with further analyses).

3)Our Haswell machine had 4 CPUs with total of 72 cores.  It was
  capable of approximately 30% more FLOPS than our KNL machine
  (Tbl.2).

================

R4 is correct that FFT/Winograd implementations can be effective for
surprisingly small kernels.  Our previous FFT work was only effective
for rather large kernels because we did not adequately exploit
vectorization.  We plan to add vectorization but this is outside the
scope of the current paper.
