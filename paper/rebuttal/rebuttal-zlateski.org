We would like to thank the reviewers for extraordinary reviews.  We
believe that addressing the issues brought up will render our paper
much stronger.

Unintentionally, we had mistakenly uploaded a working draft instead of
the final manuscript.  The missing references had been added and
comments removed.

We will release the source code (MIT license) upon publication.

We agree with R1 that the difference between [25] and
https://arxiv.org/pdf/1602.06709v1.pdf should be clearly captured.
[25] provides vectorization strategies and 'guidelines' for both
serial and parallel algorithms.  It suggests specific register
blocking for certain kernel sizes.

Our forward pass' lowest level primitive employs the same
vectorization strategy, but performs computation in a different order.
This is where the similarity ends.  Our lowest level update primitive
has different blocking strategy.  Our serial algorithms performs
computation in a sophisticated way that maximizes L2 and L3 cache hit
rates.  Further, our register and L1 blocking strategy is computed
during compile--time for an arbitrary N-dimensional kernel size.

In [25] an outline of a simple fork--join algorithm is described and
briefly analyzed for specific networks (p.566/567).  In contrast, our
statically scheduled approach provides scaleable algorithm that can
handle an arbitrary network topology and arbitrary kernel sizes.

https://arxiv.org/pdf/1602.06709v1.pdf focuses on distributing
training over multiple nodes.  It uses the same serial and parallel
algorithms as in [25].

R1:

0)Intrinsics are indeed used (as described on page 4.).  We will make
  that clearer.

a)We will mention that MKL-DNN is only optimized for forward and AVX2.

b)Using 1 vs 2 threads depends on whether it is more beneficial to
  hide latency or have more cache per thread available.  Here, maby
  factors such as blocking size, kernel sizes, available cache, etc...
  come in play.  For this reason, we decide to empirically determine
  the optimal choice.

c)Similar performances are obtained for both 7x7 and 5x5 of Googlenet
  and Resnet.  We'll add an extra "toy-layers" to cover that cases.

d)Extra analysis of the benchmarks will be added in the manuscript.
  Generally NUMA affects to the scalability for some cases on Haswell.
  KNL has linear scalability in nearly all cases, with the exception
  of Unet update, which is due to relatively low amount of FLOPS
  required and subdivision into non--exactly--equal size problems.

  Authors disagree that GEMM scales better.  On our NUMA Haswell
  machine GEMM scales worse.  On KNL our algorithm scales linearly in
  nearly all cases, with the exception of UNet which can be considered
  a harder case.  Similarly, GEMM has hard cases for which it doesn't
  scale well (small or narrow matrices).

  One of the strengths of our parallelization method is that no
  special handling of NUMA is required.  Good performances on our NUMA
  Haswell machine is due to efficient utilization of L3 cache.

e)Insufficient vectorization, not enough loop unrolling and lack of L3
  cache all contribute to relatively lower utilization of KNL versus
  Haswell.  We expect the first two problems to be solved as
  compilers' optimization of KNL code matures.  We still expect CPUs
  with L3 cache to have higher utilization on average.

f)Such analysis would be useful, however MKL-DNN is under active
  development, and it promises to be as efficient as MKL2017.  Lack of
  clear documentation or publication on the exact approach render such
  analysis by the authors of this paper hard.  We hope MKL-DNN authors
  will perform such analysis.

g)Our approach can be implemented using JIT (as in MKL-DNN), as well
  as with the c++ metaprogramming.  Moreover, our approach can be seen
  as a JIT approach as well, with C++ compiler doing the JIT
  compilation.

  However, Using c++ metaprogramming has multiple advantages:

  Single source code can be compiled for various machines.  This makes
  the code easier to maintain.  Porting to a new generation of a CPU
  or different platform requires changing 6 macros and 3 constants.
  Amazing work done by the compiler community will make sure the
  instructions are optimally scheduled.  JIT proposed in [NNPACK, MKL]
  is much harder to port without replicating much of the work done by
  the compiler community - such as optimal register usage, instruction
  scheduling, etc..

h)Unfortunately NNPack is not scaleable to high number of cores.
  (experiments and direct conversation with the author).  The authors
  believe that winograd implementations can be beneficial for the
  forward pass of networks with specific kernel sizes; however
  comparisons to such specific algorithm is outside of the scope of
  this paper.

i)We plan to publish results for DeepBench on different CPU
  generations (SSE-AVX512) after the release of the code.

R2:

1)Superficially, similar approaches are used for GEMM kernels, such as
  register and cache blocking.  Moreover, the propagation algorithms
  can be augmented to GEMM.  Such approach, however, introduces both
  memory and/or computation overheads [5].... THIS HAS TO BE FINISHED

2)As mentioned above, the authors will add extra analisys on how
  different architectures affect performance (scalability).  However,
  the authors believe that scalability achieved (Fig.5) can be
  considered beneficial for all networks.

3)Our Haswell machine had 4 CPUs with total of 72 cores.  It was
  capable of approximately 30% more FLOPS than our KNL machine
  (Tbl.2).

We promise to work hard on improving the narrative, especially the
paragraphs describing the details of the algorithm.
