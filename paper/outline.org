* Compile-time optimized N-D ConvNet Primitives for Xeon Phi Many-Core CPU (and regular Xeon ?)
** Short Abstract
** Abstract

   We focus on 2D and 3D, however the same approach can be applied to
   ND problem.

   We propose an algorithm for CNN layer primitives.  Layer
   computation is broken up into tasks, for which the code is
   generated during compile time using C++'s template metaprogramming
   machinery.  An empirically fastest execution plan is then created
   during runtime (once per computer, can be stored in a knowledge
   file), and can be reused just as in the FFTW's case.

   The execution plan is based on a static task execution scheduler
   which is designed to optimize the cache locality and evenly
   distribute the work over the available cores.

   The edges of the graph represent the overhead (or 'inverse' the
   cache reuse) for having the two tasks executed in sequence.

   We show that our sub-tasks achieve very high % of the CPU
   utilization for both the Phi and regular Xeon.  Our static
   scheduler yields nearly linear speedup on the 64 core Xeon Phi and
   regular Xeon CPUs.

   We compare the speed, and utilization against publicly available
   implementations for both the CPU and the GPU.

   We show that our algorithm achieves consistent performance
   regardless of the ConvNet topology.  On average, our approach
   utilizes much higher % of peak FLOPS performances compared to the
   alternatives.

** Intro
*** Explain the Xeon Phi architecture

   Phi has only two levels of cache.  L2 is shared per two physical
   cores and is 1M.  Each core has 32K of L1.  Cache is layered on a
   grid, but we do not actually have problem of dirty cache
   propagation (explain why).

   Even though our approach is optimized for the two level cache of
   XeonPhi, yields high performances on regular Xeon as well (which
   has lower memory throughput but another level [L3] cache).


*** Related work

    - cuDNN for the GPU
    - (https://github.com/01org/idlf) canceled intel project that uses
      JIT, this is where we claim that templates are better/easier
      than JIT

** Claims
*** With small kernels direct convolution is preferred
**** FFT can be more efficient for just inference, but its memory bound
*** Layer computation can be broken up into compute bound tasks
**** Which can achieve high CPU utilization

     XX % of the peak FLOP/s represent useful computation

*** Novel static scheduling yields linear speedup on the Xeon Phi TM
** Problems to solve
*** Efficient use of the FMA units

    Main goal is to have most instruction operate on two registers and
    one L1 memory address.

    Use register blocking (cite) so that two FMA instructions can be
    issues every cycle.  Use L1 blocking (cite) for the third argument

*** Compile time loop unrolling and index calculation

    Two possible approaches

    1) JIT - intel's approach
    2) C++ template metaprogramming - need to be recompiled

       We explain why the need for re-compilation is not an issue.

       We choose the second approach.  C++'s templates are turing
       complete + we can rely on the compiler for additional
       optimizations.

       Plus, no knowledge of assembly required (only using intrinsics)

       We show how both GCC and ICC generate highly efficient code
       (this will have a code snipped and a snipped of the generated
       assembly code).

*** Cache locality

    And hardware pre-fetch friendly code.  Prefer linear memory
    traverse.


*** Parallelization

    Assumptions:

    1) No NUMA - this can be addressed later
    2) CPUs exclusively available.
    3) No speed stepping or turbo boost

    Separate the problem into compute bound subproblems (register
    blocking + L1 blocking).

    Generate a graph of tasks with edges represeting the number of
    cache reuse betwen the tasks.

    Statically schedule tasks to maximize the cache re-use.
** Algorithms

*** Data in memory

    SW = SIMD width, which is 16 floats for KNL and 8 floats for AVX/AVX2

**** Images

    x,y,z location of f-th featuremap of the batch b is in the array
    location

    Im[ b ][ f / SW ][ Z ][ Y ][ X ][ f % SW]

    All arrays are aligned to the cache line.  Custom strides allowed
    as long as Im[ A ][ B ][ C ][ D ] is cache aligned.

**** Kernels

     x,y,z location of f -> f' kernel is in the array location

     W[ f / SW ][ f' / SW ][ Z ][ Y ][ X ][ f % SW ][ f' % SW ]

*** Serial

**** Forward/Backward convolution sub-task pseudocode

    Divide the output into RBDxRBHxRBW blocks (RB stands for register blocking)

    // load into registers
    FOR rbd = 0 to RBD-1 DO
      FOR rbh = 0 to RBH-1 DO
        FOR rbw = 0 to RBW-1 DO
          CONDITIONAL-LOAD O[rbd][rbh][rbw][:]
        END FOR
      END FOR
    ENDFOR

    FOR kd = 0 to KD-1 DO
      FOR kh = 0 to KH-1 DO
        FOR kw = 0 to KW-1 DO
          FOR f = 0 to F DO
            KELEM = LOAD Kernel[kd][kh][kw][f][:]

            FOR rbd = 0 to RBD-1 DO
              FOR rbh = 0 to RBH-1 DO
                FOR rbw = 0 to RBW-1 DO
                  O[rbd][rbh][rbw][:] = FMADD( KELEM[:], SET1(I[rbd+kd][rbh+kh][rbw+kw][f]), O[rbd][rbh][rbw][:] )

    FOR rbd = 0 to RBD-1 DO
      FOR rbh = 0 to RBH-1 DO
        FOR rbw = 0 to RBW-1 DO
          STORE O[rbd][rbh][rbw][:]
        END FOR
      END FOR
    ENDFOR

    RBD x RBH x RBW + 1 <= # of ZMM/YMM registers

**** Update convolution sub-task

    Divide G into L1D x L1H x L1W

    Divide kernels into RBD x RBH x RBW x RBF

    CONDITIONAL-LOAD W'[RBD][RBH][RBW][RBF][:]

    FOR l1d = 0 to L1D - 1, FOR l1h....
      X = G[l1d][l1h][l1w][:]
      FOR rbd, rbh, rbw, rbf...
        W[rbd][rbh][rbw][rbf][:] = FMADD( X[l], SET1(I[rbd+l1d][rbh+l1h][rbw+l1w][rbf]), W[rbd][rbh][rbw][rbf][:] );

    STORE W'[RBD][RBH][RBW][RBF][:]

*** Parallel

** Experiments
*** Single thread utilization on different architectures

  Preliminary results (FLOPs utilization):

  |                           | AlexNet | VGG (Oxford) | VD2D3D |
  |---------------------------+---------+--------------+--------|
  | KNL (fwd/bwd)             |  40-50% |       50-60% | 50-60% |
  | KNL (update)              |  55-70% |       65-75% | 65-75% |
  | Xeon (fwd/bwd)            |  75-80% |       75-80% | 75-80% |
  | Xeon (update)             |  80-85% |       80-85% | 80-85% |
  | AVX fwd/bwd (old macbook) |     70% |          80% |    80% |
  | AVX update (old macbook)  |     75% |          80% |    80% |


*** Parallelization - linear speedup for batch and patch training?

    ~80% on KNL (to 60 cores), 90% on Xeon ( to 18 cores ).

*** Comparison with other frameworks
**** Famous 2D networks (should include VGG)
**** Some 3D networks (our choice? VD2D3D)
** Contributions
