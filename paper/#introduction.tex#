\section{Introduction}

  ConvNets had become... popular \mynote{seung}{intro about convnets}.
  The argument of using CPUs or GPUs for deep learning had reached
  almost religious proportions.  The GPUs, which are capable of more
  FLOPs/s seem to win the battle by a large margin -- disproportional
  to the FLOPs/s available to the two architectures.  We find this
  absurd, as the CPUs have been around for much longer, and the CPU
  programming model changes very little from a generation to the next.
  On the other side, GPUs seem to change a lot.

  As the CPUs are closing the gap in terms of FLOPs/s performance.
  Xeon Phi Knights Landing chips are capable of up to
  6TFLOPs/s~\cite{}.  Upcoming Skylake server CPU's will be packing
  $3.2$ TFLOPs/s per single chip, delivering up to $25.8$ TFLOPs/s on
  an 8-way configuration~\cite{}.  We believe that the main battle
  should be in harvesting the computational power of the hardware.  We
  are surprised to see that the GPU implementations have higher
  utilization of the \% FLOPs/s available, compared to the CPU.

  Reducing convolution to matrix multiplication has been a popular
  approach due to the availability of highly efficient matrix
  multiplication routines.  This approach has, however, both a
  computational and memory overhead due to the requirement of creating
  in--memory matrices.  Caffe con Troll (CCT)~\cite{hadjis2015shallow}
  uses blas to perform convolutions -- we will compare us to them.

  Initially, cuDNN~\cite{chetlur2014cudnn} took that approach,
  however, it has since implemented more efficient direct convolution
  algorithms.

  Convolutional layers of a ConvNet are computationally dominating
  both training and inference.  The forward and backward pass involve
  a convolution of an image with a small kernel.  Typically ``valid''
  dense convolution, possibly with strides is performed.  More
  advanced types of convolutions can decomposed to a series of dense
  convolutions~\cite{szegedy2015going}.

  We propose a new approach.  We harvest the power of C++ templates to
  create compile time primitives by harvesting the compiler.  Given
  the specifications of the CPU, our meta--algorithms optimize and
  vectorize single threaded code, as well as generate static
  scheduling for parallelizing the computation over multiple cores.

  We show that our approach can utilize extremely high percentage of
  the available FLOPS regardless of the CPU generation, as well as
  have near linear scalability over the number of cores available.
  The performances of our algorithms are competitive with the state of
  the art hand optimized code for specific architectures.  Our code
  yields better performances on the KNL compared to the state of the
  art GPU implementations for 3D convolutional networks, even though
  KNL has less FLOPS/s available.

  We further demonstrate the power of our approach for inference only
  computation.

  Even though we provide an efficient, publicly available, open source
  implementation for popular ConvNet primitives, the goal of this
  paper is to point to the right direction for solving the problem,
  rather than giving the final ``optimized'' implementation.

  Efficient algorithms for training and inference of ConvNets with
  larger kernels have been introduced in
  ~\cite{zlateski2016znn,zlateski2016znni}, however the evidence
  suggests that ConvNets with smaller kernels, and larger number of
  layers perform better on nearly all problems. (Cite).

  %% \subsection{GPUs vs CPUs}

  %% GPUs have streaming multiprocessors, while the CPUs have in--order,
  %% memory coherent cores.

  %% GPUs model is so called SIMT (single instruction multiple
  %% threads)... dwell on it..

  %% CPUs on the other side have a complex structure of many cores and
  %% virtual cores, many cache layers and each core capable of performing
  %% SIMD instructions on up to 16 floating point numbers.

  %% In this paper we are not trying to answer the GPU vs CPU question,
  %% but rather propose an algorithm and implementation for ConvNet
  %% primitives that can utilize very high percentage of the theoretical
  %% peak performance of both many--core (Xeon Phi) and multi--core
  %% (Xeon) CPUs.  As shown later, our algorithm performs well on
  %% multi--chip systems as well.

  %% \subsection{Mulit--core vs Many--core}

  %% Explain the differences NUMA, caches, etc..

  \subsection{Motivation}

  Modern CPUs can achieve high performances under very special
  circumstances.  Mainly, each core of modern CPUs has up to two FMA
  vector units that are capable of performing $2S$ floating point
  instructions per cycle.  Here, $S$ is the width of the vector
  register (how many floating point numbers fit inside the register).
  The peak performace of a single core is then reached when, in each
  cycle, all available FMA units perform a fused multiply--add
  operation whith the input stored in the register file.
  Additionally, each CPU has a different latency of the FMA units.
  This means that the result of the computation is not immidiately
  available, but rather after $c$ cycles, where $c$ is the latency of
  the unit.  To fully utilize the $n$ FMA units, a program must
  continiously issue FMA instructions such that no $nc$ consecutive
  instructions are dependent (a result of one instruction is an input
  for some other one).  Modern compilers are aware of these
  limitations, and will try to assemble the code in the most optimal
  way.

  Loading data from memory to a register introduces an additional
  overhead.  Thus, to achieve high performance, one must minimize such
  data transfers and re--use data in the register file as much as
  possible.  Furthermore, fast loading of data from memory requires
  additional constraints.  Fast loading $S$ values into a vector
  register requires the values to be consecutive and properly aligned
  in memory.

  Efficiently utilizing CPUs with multiple cores introduce an
  additional constraint.  In order to fully utilize all available
  cores, they must perform independent computation and minimize
  synchronization.  A core shouldn't wait for a result computed by
  another core.

  Because of these constraints, the naive way of computing
  convolutions is very inefficient.  Designing an algorithm that
  follows all the constraints is challenging.  To design such
  algorithm, we are motivated by the following two principles.  First,
  we divide computation into small sub--tasks.  We provide a
  relatively simple implementation for each task so that the compiler
  can generate optimal machine code.  Secondly, the computation will
  be evenly distributed among the available cores, such that each core
  does an equal amount of work.  This minimal synchronization is
  necessary and each core can start and ends the computation at the
  same time.
