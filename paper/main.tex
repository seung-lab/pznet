\tolerance=10000

\documentclass[conference]{./IEEEtran/IEEEtran}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{breakurl}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{eqnarray}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{multirow}
\def\UrlBreaks{\do\/\do-}

\newcommand*\rot[1]{\rotatebox[origin=c]{90}{#1}}

\usepackage{ifthen}
\usepackage{color}
\usepackage{xcolor}
\usepackage[]{algorithm}
\usepackage{clrscode4e}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{url}

\newboolean{showcomments}
\setboolean{showcomments}{true}

\ifthenelse{\boolean{showcomments}}
           { \newcommand{\mynote}[3] {
               \fbox{\bfseries\sffamily\scriptsize#1}
                    {\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}}
           { \newcommand{\mynote}[3]{}}

\newcommand{\aleks}[1]{\mynote{Aleks}{#1}{violet}}
\newcommand{\seung}[1]{\mynote{Seung}{#1}{blue}}
\newcommand{\kisuk}[1]{\mynote{Kisuk}{#1}{green}}

\newcommand{\remove}[1]{}


\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\angled}{\langle}{\rangle}


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\title{Compile--Time Optimized and Statically Scheduled N--D ConvNet
  Primitives for Multi--Core and Many--Core (Xeon Phi) CPUs }


\author{\IEEEauthorblockN{Aleksandar Zlateski\IEEEauthorrefmark{1}, H
    Sebastian Seung\IEEEauthorrefmark{2}} \IEEEauthorblockA{Princeton
    Neuroscience Institute\\ and Computer Science
    Department\\ Princeton University\\ Princeton, NJ 08540
    USA\\ \IEEEauthorrefmark{1}{\tt zlateski@princeton.edu},
    \IEEEauthorrefmark{2}{\tt sseung@princeton.edu}}} \maketitle



\begin{abstract}

  With the increasing computation power of hardware, convolutional
  networks (ConvNets), branded as deep--learning, became the most
  popular approach to computer vision.  The vast majority of the
  computations performed during both training and using trained
  ConvNet consists of convolutions.  Being capable of more FLOPS, GPUs
  have been a hardware of choice for ConvNets.  As the CPUs are
  closing the FLOPS gap, it is important to have efficient CPU
  algorithms.  We propose a novel parallel and vectorized algorithm
  for N--D convolutional layers.  Our algorithm is designed to be
  agnostic to both the ConvNet architecture and CPU capabilities, such
  as vector units, number of cores or cache sizes.  To achieve high
  utilization, we assume that the network architecture will be known
  at compile--time.  Our serial algorithm divides the computation into
  small sub--tasks designed to be easily optimized by the compiler for
  specific CPU.  The sub--tasks are executed in an order that
  maximizes cache reuse.  We parallelize the algorithm by statically
  scheduling tasks to be executed by each core.  Our novel
  compile--time recursive scheduling algorithm is capable of dividing
  the computation evenly among an arbitrary number of cores,
  regardless of the network architecture.  It introduces zero runtime
  overhead and minimal synchronization overhead.  We demonstrate that
  our serial primitives consistently achieve very high utilization of
  available FLOPS (75-95\%) while our parallel algorithm attains
  50-90\% utilization on 64+ core machines.  For 2D image recognition
  ConvNets, our algorithm is competitive to the currently fastest CPU
  implementation (MKL2017), while it greatly outperforms for image
  segmentation ConvNets.  Finally, for 3D networks we show competitive
  performances to the latest cuDNN libraries running on latest GPU
  hardware, while having only half the FLOPS available.

\end{abstract}
\setlength{\belowcaptionskip}{-15pt}

\input{introduction}
\input{convlayer}
\input{static}
\input{fwd-bwd}
\input{upd}
%\input{serial}
%\input{scheduler}
\input{benchmarks}
\input{conclusion}
%\input{1dconvnet}

\clearpage
\bibliographystyle{./IEEEtran/IEEEtranBST/IEEEtran}
\bibliography{IEEEabrv,znnphi}
\end{document}
