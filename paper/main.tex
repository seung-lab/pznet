\tolerance=10000

\documentclass[conference]{./IEEEtran/IEEEtran}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{breakurl}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{eqnarray}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{multirow}
\def\UrlBreaks{\do\/\do-}

\newcommand*\rot[1]{\rotatebox[origin=c]{90}{#1}}

\usepackage{ifthen}
\usepackage{color}
\usepackage{xcolor}
\usepackage[]{algorithm}
\usepackage{clrscode4e}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{url}

\captionsetup[algorithm]{font=footnotesize}

\newboolean{showcomments}
\setboolean{showcomments}{true}

\ifthenelse{\boolean{showcomments}}
           { \newcommand{\mynote}[3] {
               \fbox{\bfseries\sffamily\scriptsize#1}
                    {\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}}
           { \newcommand{\mynote}[3]{}}

\newcommand{\aleks}[1]{\mynote{Aleks}{#1}{violet}}
\newcommand{\seung}[1]{\mynote{Seung}{#1}{blue}}
\newcommand{\kisuk}[1]{\mynote{Kisuk}{#1}{green}}

\newcommand{\remove}[1]{}


\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\angled}{\langle}{\rangle}
\DeclarePairedDelimiter{\sqb}{\big[}{\big]}


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\title{Compile--Time Optimized and Statically Scheduled N--D ConvNet
  Primitives for Multi--Core and Many--Core (Xeon Phi) CPUs }


\author{\IEEEauthorblockN{Aleksandar Zlateski\IEEEauthorrefmark{1}, H
    Sebastian Seung\IEEEauthorrefmark{2}} \IEEEauthorblockA{Princeton
    Neuroscience Institute\\ and Computer Science
    Department\\ Princeton University\\ Princeton, NJ 08540
    USA\\ \IEEEauthorrefmark{1}{\tt zlateski@princeton.edu},
    \IEEEauthorrefmark{2}{\tt sseung@princeton.edu}}} \maketitle



\begin{abstract}

  Convolutional networks (ConvNets), largely running on GPUs, have
  become the most popular approach to computer vision.  Now that CPUs
  are closing the FLOPS gap with GPUs, efficient CPU algorithms are
  becoming more important.  We propose a novel parallel and vectorized
  algorithm for N--D convolutional layers.  Our goal is to achieve
  high utilization of available FLOPS, independent of ConvNet
  architecture and CPU properties (e.g. vector units, number of cores,
  cache sizes). Our approach is to rely on the compiler to optimize
  code, thereby removing the need for hand-tuning.  We assume that the
  network architecture is known at compile--time. Our serial algorithm
  divides the computation into small sub--tasks designed to be easily
  optimized by the compiler for a specific CPU.  Sub--tasks are
  executed in an order that maximizes cache reuse.  We parallelize the
  algorithm by statically scheduling tasks to be executed by each
  core.  Our novel compile--time recursive scheduling algorithm is
  capable of dividing the computation evenly between an arbitrary number
  of cores, regardless of ConvNet architecture.  It introduces
  zero runtime overhead and minimal synchronization overhead.  We
  demonstrate that our serial primitives efficiently utilize available
  FLOPS (75-95\%), while our parallel algorithm attains 50-90\%
  utilization on 64+ core machines. Our algorithm is competitive with
  the fastest CPU implementation to date (MKL2017) for 2D object
  recognition, and performs much better for image segmentation.
  For 3D ConvNets we demonstrate comparable performance to the
  latest GPU hardware and software even though the CPU is only capable
  of half the FLOPS of the GPU.

\end{abstract}
\setlength{\belowcaptionskip}{-10pt}

\input{introduction}
\input{convlayer}
\input{static}
\input{problem}
\input{data-layout}
\input{fwd-bwd}
\input{upd}
\input{benchmarks}
\input{conclusion}

%\clearpage
\bibliographystyle{./IEEEtran/IEEEtranBST/IEEEtran}
\bibliography{IEEEabrv,znnphi}
\end{document}
