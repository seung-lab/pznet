% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{hadjis2015shallow}
S.~Hadjis, F.~Abuzaid, C.~Zhang, and C.~R.~C. con Troll, ``Shallow ideas to
  speed up deep learning,'' in \emph{Workshop on Data analytics in the Cloud
  (DanaC)}, 2015.

\bibitem{chetlur2014cudnn}
S.~Chetlur, C.~Woolley, P.~Vandermersch, J.~Cohen, J.~Tran, B.~Catanzaro, and
  E.~Shelhamer, ``cudnn: Efficient primitives for deep learning,'' \emph{arXiv
  preprint arXiv:1410.0759}, 2014.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
  \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2015, pp. 1--9.

\bibitem{zlateski2016znn}
A.~Zlateski, K.~Lee, and H.~S. Seung, ``Znn - a fast and scalable algorithm for
  training 3d convolutional networks on multi-core and many-core shared memory
  machines,'' in \emph{Proceedings of the 2016 IEEE International Parallel \&
  Distributed Processing Symposium}, ser. IPDPS '16, 2016, pp. 801--811.

\bibitem{zlateski2016znni}
------, ``Znni - maximizing the inference throughput of 3d convolutional
  networks on multi-core cpus and gpus,'' \emph{arXiv preprint
  arXiv:1606.05688}, 2016.

\bibitem{jeffers2016knl}
J.~Jeffers, J.~Reinders, and A.~Sodani, \emph{Intel Xeon Phi Processor High
  Performance Programming: Knights Landing Edition, Edition 2}.\hskip 1em plus
  0.5em minus 0.4em\relax Morgan Kaufmann, 2016.

\bibitem{cpu-myth}
``Myth busted: General purpose cpus canâ€™t tackle deep neural network
  training,''
  \url{http://itpeernetwork.intel.com/myth-busted-general-purpose-cpus-cant-tackle-deep-neural-network-training/}.

\bibitem{reinders2007intel}
J.~Reinders, \emph{Intel threading building blocks: outfitting C++ for
  multi-core processor parallelism}.\hskip 1em plus 0.5em minus 0.4em\relax "
  O'Reilly Media, Inc.", 2007.

\bibitem{willhalm2008putting}
T.~Willhalm and N.~Popovici, ``Putting intel{\textregistered} threading
  building blocks to work,'' in \emph{Proceedings of the 1st international
  workshop on Multicore software engineering}.\hskip 1em plus 0.5em minus
  0.4em\relax ACM, 2008, pp. 3--4.

\bibitem{jeffers2015high}
J.~Jeffers and J.~Reinders, \emph{High Performance Parallelism Pearls Volume
  Two: Multicore and Many-core Programming Approaches}.\hskip 1em plus 0.5em
  minus 0.4em\relax Morgan Kaufmann, 2015.

\bibitem{idlf}
``The intel(r) deep learning framework,'' \url{https://github.com/01org/idlf}.

\bibitem{mkl-dnn}
``Intel(r) math kernel library for deep neural networks,''
  \url{https://github.com/01org/mkl-dnn}.

\bibitem{neonnervana}
Nervana, ``Neon,'' \url{https://github.com/NervanaSystems/neon}.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for
  biomedical image segmentation,'' in \emph{International Conference on Medical
  Image Computing and Computer-Assisted Intervention}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2015, pp. 234--241.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{sermanet2013overfeat}
P.~Sermanet, D.~Eigen, X.~Zhang, M.~Mathieu, R.~Fergus, and Y.~LeCun,
  ``Overfeat: Integrated recognition, localization and detection using
  convolutional networks,'' \emph{arXiv preprint arXiv:1312.6229}, 2013.

\bibitem{lee2015recursive}
K.~Lee, A.~Zlateski, A.~Vishwanathan, and H.~S. Seung, ``Recursive training of
  2d-3d convolutional networks for neuronal boundary detection,'' \emph{arXiv
  preprint arXiv:1508.04843}, 2015.

\bibitem{jain2007supervised}
V.~Jain, J.~F. Murray, F.~Roth, S.~Turaga, V.~Zhigulin, K.~L. Briggman, M.~N.
  Helmstaedter, W.~Denk, and H.~S. Seung, ``Supervised learning of image
  restoration with convolutional networks,'' in \emph{Computer Vision, 2007.
  ICCV 2007. IEEE 11th International Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2007, pp. 1--8.

\bibitem{jia2014caffe}
Y.~Jia, E.~Shelhamer, J.~Donahue, S.~Karayev, J.~Long, R.~Girshick,
  S.~Guadarrama, and T.~Darrell, ``Caffe: Convolutional architecture for fast
  feature embedding,'' in \emph{Proceedings of the ACM International Conference
  on Multimedia}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2014, pp.
  675--678.

\bibitem{viebke2015potential}
A.~Viebke and S.~Pllana, ``The potential of the intel xeon phi for supervised
  deep learning,'' \emph{arXiv preprint arXiv:1506.09067}, 2015.

\end{thebibliography}
