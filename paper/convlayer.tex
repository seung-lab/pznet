\section{Convolutional layers} \label{sec-conv-layers}
  The convolution of two $N$-D images $A$ and $B$ will be written as
  $A*B$.  The cross-correlation $A\star B$ will be defined as the
  convolution $A\ast\bar{B}$, where $\bar{B}$ is the $N$-D
  image obtained by reflecting $B$ in all $N$ directions (reflection
  through the origin). We follow the convention that all
  cross-correlations are \texttt{valid}, meaning that the output is
  computed only for configurations in which $A$ is fully contained in
  $B$ or vice versa.  For instance, if the input image has size $n^3$
  and the kernel has size $k^3$, then the output image has size $n'^3
  = (n-k+1)^3$.  We follow the convention that all convolutions are
  \texttt{full}, meaning that the output is calculated for all
  possible overlapping configurations of $A$ and $B$.

  In the forward phase, a convolutional layer transforms an input
  tuple of $F$ images into an output tuple of $F'$ images.  We want to
  process a batch of $B$ inputs to yield a batch of $B$ outputs, via
  \begin{equation}\label{eq:forward}
  I'_{b,j} = \sum_{i=1}^F I_{b,i}\star W_{ji} + \theta_j
  \end{equation}
  for $1 \le b \le B$ and $1 \le j \le F'$.  Here $I_{b,i}$ is the
  $i^{th}$ image of the $b^{th}$ input in the batch, and $I'_{b,j}$ is
  the $j^{th}$ image of the $b^{th}$ output in the batch, and $W_{ji}$
  is the kernel from the $i^{th}$ image in an input tuple to the
  $j^{th}$ image in an output tuple.  Typically, a scalar ``bias''
  $\theta_j$ is added to each pixel of the output image $j$.

  In the backward phase, a convolutional layer transforms a tuple of
  $F'$ images into a tuple of $F$ images.  We process a batch of $B$
  tuples via
  \begin{equation}\label{eq:backward}
  \hat{I}_{b,i} = \sum_{j=1}^{F'} \hat{I}'_{b,j} \ast W_{ji}
  \end{equation}
  for $1 \le b \le B$ and $1 \le i \le F$.  $\hat{I}$ and $\hat{I}'$
  are batches of image tuples with the same size as $I$ and $I'$ in
  Eq. (\ref{eq:forward}).

  In the update phase, we calculate the kernel updates for the
  convolutional layer
  \begin{equation}\label{eq:update}
  \Delta W_{j,i} = \sum_{b=1}^B I_{b,i} \star \hat{I}'_{b,j}
  \end{equation}
  for $1 \le i \le F$ and $1 \le j \le F'$.  The kernel updates
  $\Delta W_{j,i}$ are images of the same size as the kernels
  $W_{j,i}$.

  Here we focus on computing the convolutions and cross--correlations
  of Eqs. (\ref{eq:forward})-(\ref{eq:update}) efficiently.  The
  meanings of these equations in the context of backpropagation
  learning are not required for understanding the contributions of
  this paper.  (In brief, $\hat{I}$ is the gradient of the loss
  function for learning with respect to $I$, and $\Delta W_{j,i}$ is
  the direction along which the kernels are modified by the learning.)

%  Gradients of the loss with respect to the input/output/kernels are
%  as well $N$--dimensional images of the same size as the
%  input/output/kernels.  For simplicity, in the rest of the writeup we
%  will refer to them simply as input/output/kernel gradients and use
%  notation of $GI, GI', GW$.

%  Performing \texttt{full}
%  cross--correlation (or convolution) can be accomplished by
%  performing \texttt{valid} convolution on a zero--padded image.
%  Thus, same algorithm can be used for both the forward and the
%  backward pass.

  For the sake of concreteness, we will assume 3D images and
  kernels. The generalization to $N-D$ images should be
  straightforward to the reader.
  If $I_{b,i}$ has size $\vec{N} = \angled{N_x, N_y, N_z}$ and
  $W_{ji}$ has size $\vec{K} = \angled{K_x,K_y,K_z}$, then we can
  regard $I$ and $\hat{I}$ as 5D tensors of size $B \times F \times N_x
  \times N_y \times N_z$, $W$ and $\Delta W$ as 5D tensors of size $F'
  \times F \times K_x \times K_y \times K_z$, and $I'$ and $\hat{I'}$ as
  5D tensors of size $B \times F' \times N_x' \times N_y' \times N_z'$,
  where $\vec{N}' = \vec{N} - \vec{K} + \vec{1}$.
