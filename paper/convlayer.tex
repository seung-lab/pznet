\section{Convolutional layers} \label{sec-conv-layers}

  We quickly summarize the computation performed by the convolutional
  layers.  In the forward propagation pass of a convolutional layer a
  tuple of $F$ images are transformed into another tuple of $F'$
  images.  We want to process a batch of $B$ inputs to yield a batch
  of $B$ outputs, via
  \[
  I'_{b,j} = \sum_{i=1}^F I_{b,i} \star W_{ji}
  \]
  for $1 \le b \le B$ and $1 \le j \le F'$.  Here $I_{b,i}$ is the
  $i^{th}$ image of the $b^{th}$ input in the batch, and $I'_{b,j}$ is
  the $j^{th}$ image of the $b^{th}$ output in the batch, and $W_{ji}$
  is the kernel from the $i^{th}$ image in an input tuple to the
  $j^{th}$ image in an output tuple.

  In the backward propagation pass, a tuple of $F'$ gradients of the
  loss with respect to the outputs are transformed into a tuple of $F$
  gradients of the loss with respect to the input.  We process a batch
  of $B$ tuples, via
  \[
  \frac{\partial L}{\partial I_{b,i}} = \sum_{j=1}^{f'}
  \frac{\partial L}{\partial I'_{b,j}} \ast W_{ji}
  \]
  for $1 \le b \le B$ and $1 \le i \le F$.  Here $\frac{\partial
    L}{\partial I'_{b,j}}$ is the $j^{th}$ gradient of the loss with
  respect to the output of the $b^{th}$ input in the batch, and
  $\frac{\partial L}{\partial I_{b,i}}$ is the $i^{th}$ gradient of
  the loss with respect to the input of the $b^{th}$ input in the
  batch.

  Finally, in the update phase, the gradients of the loss with respect
  to the kernels are computed via,
  \[
  \frac{\partial L}{\partial W_{j,i}} = \sum_{b=1}^B
  \frac{\partial L}{\partial I'_{b,j}}  \star I_{b,i}
  \]
  for $1 \le i \le F$ and $1 \le j \le F'$.

  In the forward pass and the update phase, so called \texttt{valid}
  discrete cross--correlation is performed.  The output of a
  \texttt{valid} cross--correlation $A \star B$ is computed only for
  the locations where $A$ is fully contained in $B$ or vice versa.
  For instance, if the input image has size $n^3$ and the kernel has
  size $k^3$, then the output image has size $n'^3 = (n-k+1)^3$.  In
  the backward pass \texttt{full} discrete convolution is performed.
  The output is calculated for all locations of the kernel that have
  some overlap with the input.

  Here, we focus on computing these convolutions and
  cross--correlations efficiently.  No further knowledge of the roles
  of the values computed during the three passes are required to
  understand the problem and the contributions in this paper.

  Gradients of the loss with respect to the input/output/kernels are
  as well $N$--dimensional images of the same size as the
  input/output/kernels.  For simplicity, in the rest of the writeup we
  will refer to them simply as input/output/kernel gradients and use
  notation of $GI, GI', GW$.

  Convolving of an image $I$ with a kernel $W$ is equivalent to
  cross--correlating it with the transposed kernel ($W^T$).  A
  transposed $N$--dimensional image is an image that is reflected
  along all $N$ dimensions.  Performing \texttt{full}
  cross--correlation (or convolution) can be accomplished by
  performing \texttt{valid} convolution on a zero--padded image.
  Thus, same algorithm can be used for both the forward and the
  backward pass.

  We will assume 3D images and kernels, even though our approach is
  generalizable to ND images.  The choice of 3D was made because we
  believe that it provides enough insight to the reader to recognize
  the patterns in our algorithms' design, allowing him to extend the
  algorithms to an arbitrary number of dimensions.  We think that
  describing the generic algorithm would make the..... FINISH THIS


  If $I_{b,i}$ has size $\vec{N} = \angled{N_x, N_y, N_z}$ and
  $W_{ji}$ has size $\vec{K} = \angled{K_x,K_y,K_z}$, then we can
  regard $I$ and $GI$ as a 5D tensors of size $B \times F \times N_x
  \times N_y \times N_z$, $W$ and $GW$ as a 5D tensor of size $F'
  \times F \times K_x \times K_y \times K_z$, and $I'$ and $GI'$ as a
  5D tensor of size $B \times F' \times N_x' \times N_y' \times N_z'$,
  where $\vec{N}' = \vec{N} - \vec{K} + \vec{1}$.
