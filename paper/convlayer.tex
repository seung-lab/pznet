\section{Convolutional layers} \label{sec-conv-layers}
  The convolution of two $N$-D images $A$ and $B$ will be written as
  $A*B$.  The cross-correlation $A\star B$ will be defined as the
  convolution $A\ast\bar{B}$, where $\bar{B}$ is the $N$-D
  image obtained by reflecting $B$ in all $N$ directions (reflection
  through the origin). We follow the convention that all
  cross-correlations are \texttt{valid}, meaning that the output is
  computed only for configurations in which $A$ is fully contained in
  $B$ or vice versa.  For instance, if the input image has size $n^3$
  and the kernel has size $k^3$, then the output image has size $n'^3
  = (n-k+1)^3$.  We follow the convention that all convolutions are
  \texttt{full}, meaning that the output is calculated for all
  possible overlapping configurations of $A$ and $B$.

  In the forward phase, a convolutional layer
  transforms an input tuple of $F$ images into an output tuple of $F'$
  images.  We want to process a batch of $B$ inputs to yield a batch
  of $B$ outputs, via
  \begin{equation}\label{eq:forward}
  I'_{b,j} = \sum_{i=1}^F I_{b,i}\star W_{ji}
  \end{equation}
  for $1 \le b \le B$ and $1 \le j \le F'$.  Here $I_{b,i}$ is the
  $i^{th}$ image of the $b^{th}$ input in the batch, and $I'_{b,j}$ is
  the $j^{th}$ image of the $b^{th}$ output in the batch, and $W_{ji}$
  is the kernel from the $i^{th}$ image in an input tuple to the
  $j^{th}$ image in an output tuple.

  In the backward phase, a convolutional layer transforms a tuple of
  $F'$ images into a tuple of $F$ images.  We process a batch of $B$
  tuples via
  \begin{equation}\label{eq:backward}
  \hat{I}_{b,i} = \sum_{j=1}^{F'} \hat{I}'_{b,j} \ast W_{ji}
  \end{equation}
  for $1 \le b \le B$ and $1 \le i \le F$.  The meaning of
  $\hat{I}$ (gradient of the loss function for the training) is not
  important here.  All that matters is that $\hat{I}$ and $\hat{I}'$
  are batches of image tuples with the same size as $I$ and $I'$ in
  Eq. (\ref{eq:forward}).

  In the update phase, the kernels of the convolutional layer are
  modified by
  \[
  \Delta W_{j,i} = \sum_{b=1}^B I_{b,i} \star \hat{I}'_{b,j}
  \]
  for $1 \le i \le F$ and $1 \le j \le F'$.

  Here we focus on computing these convolutions and
  cross--correlations efficiently.  No further knowledge of the roles
  of the values computed during the three passes are required to
  understand the problem and the contributions in this paper.

%  Gradients of the loss with respect to the input/output/kernels are
%  as well $N$--dimensional images of the same size as the
%  input/output/kernels.  For simplicity, in the rest of the writeup we
%  will refer to them simply as input/output/kernel gradients and use
%  notation of $GI, GI', GW$.

%  Performing \texttt{full}
%  cross--correlation (or convolution) can be accomplished by
%  performing \texttt{valid} convolution on a zero--padded image.
%  Thus, same algorithm can be used for both the forward and the
%  backward pass.

  For the sake of concreteness, we will assume 3D images and
  kernels. The generalization to $N-D$ images should be
  straightforward to the reader.
  If $I_{b,i}$ has size $\vec{N} = \angled{N_x, N_y, N_z}$ and
  $W_{ji}$ has size $\vec{K} = \angled{K_x,K_y,K_z}$, then we can
  regard $I$ and $\bar{I}$ as 5D tensors of size $B \times F \times N_x
  \times N_y \times N_z$, $W$ and $\Delta W$ as 5D tensors of size $F'
  \times F \times K_x \times K_y \times K_z$, and $I'$ and $\bar{I'}$ as
  5D tensors of size $B \times F' \times N_x' \times N_y' \times N_z'$,
  where $\vec{N}' = \vec{N} - \vec{K} + \vec{1}$.

  \aleks{Introduce notation for sizes of the tensors}
